{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Training and Visualizing a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "The Decision Trees Accuracy when criterion = Gini is 100.0 % \n"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "train_X,test_X,train_y,test_y = train_test_split(iris.data,iris.target,test_size=0.3, random_state=42)\n",
    "clf1 = DecisionTreeClassifier(random_state=2).fit(train_X,train_y)\n",
    "clf2 = DecisionTreeClassifier(criterion = \"entropy\",random_state=2).fit(train_X,train_y)\n",
    "\n",
    "predict_test = clf1.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "pd_all\n",
    "\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "print(correct_amount_km)\n",
    "print(\"The Decision Trees Accuracy when criterion = Gini is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "The Decision Trees Accuracy when criterion = Entropy is 100.0 % \n"
     ]
    }
   ],
   "source": [
    "predict_test = clf2.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "pd_all\n",
    "\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "print(correct_amount_km)\n",
    "print(\"The Decision Trees Accuracy when criterion = Entropy is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision Trees parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe find that when Criterion takes the entropy and Gini, the accuracy is the same, and we choose Gini because we don't have to calculate the log index, which reduces the computer memory usage and makes the calculation a bit faster\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We find that when Criterion takes the entropy and Gini, the accuracy is the same, and we choose Gini because we don't have to calculate the log index, which reduces the computer memory usage and makes the calculation a bit faster\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize and save the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf1,out_file=dot_data)\n",
    "\n",
    "#Gini Visualization\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph[0].write_pdf(\"Gini_iris.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf2,out_file=dot_data)\n",
    "\n",
    "#Entropy Visualization\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "graph[0].write_pdf(\"Entropy_iris.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(clf1.predict_proba(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Regressionda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE：5838.571428571428\n"
     ]
    }
   ],
   "source": [
    "#Decision trees\n",
    "from sklearn.datasets import load_diabetes \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "diabetes = load_diabetes()\n",
    "train_X,test_X,train_y,test_y = train_test_split(diabetes.data,diabetes.target,test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "regressor1 = DecisionTreeRegressor(random_state=0).fit(train_X,train_y)\n",
    "result= regressor1.predict(test_X)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np \n",
    "print(f\"MSE：{mean_squared_error(result, test_y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE：2821.7385595843766\n",
      "The smaller the result, the better,So linear Regression is better\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor2 = LinearRegression().fit(train_X,train_y)\n",
    "result= regressor2.predict(test_X)\n",
    "print(f\"MSE：{mean_squared_error(result, test_y)}\")\n",
    "print(\"The smaller the result, the better,So linear Regression is better\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "The SVM without StandardScaler Accuracy is 100.0 % \n"
     ]
    }
   ],
   "source": [
    "#without StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#split\n",
    "train_X,test_X,train_y,test_y = train_test_split(iris.data,iris.target,test_size=0.4, random_state=42)\n",
    "\n",
    "#mode\n",
    "svm1 = SVC().fit(train_X,train_y) \n",
    "\n",
    "#predict\n",
    "predict_test = svm1.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "\n",
    "#accuracy\n",
    "print(correct_amount_km)\n",
    "print(\"The SVM without StandardScaler Accuracy is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "The SVM with StandardScaler Accuracy is 98.33333333333333 % \n"
     ]
    }
   ],
   "source": [
    "#with StandardScaler\n",
    "scaler = StandardScaler().fit(iris.data)\n",
    "data_scale= scaler.transform(iris.data)\n",
    "\n",
    "#split\n",
    "train_X,test_X,train_y,test_y = train_test_split(data_scale,iris.target,test_size=0.4, random_state=42)\n",
    "\n",
    "#mode\n",
    "svm1 = SVC().fit(train_X,train_y) \n",
    "\n",
    "#predict\n",
    "predict_test = svm1.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "test_y = pd.DataFrame(test_y)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "\n",
    "#accuracy\n",
    "print(correct_amount_km)\n",
    "print(\"The SVM with StandardScaler Accuracy is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn order to remove the effect of unit and scale differences between features, so that each dimensional feature is treated equally, the features need to be normalised.\\nHowever, we found that after normalisation, the accuracy rate became smaller instead, which I suspect is due to the occurrence of over-fitting.\\nThe original data had different units and large gaps between features, which would make the data more scattered.\\nAfter normalisation, the gap between features becomes smaller and overfitting can easily occur.\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In order to remove the effect of unit and scale differences between features, so that each dimensional feature is treated equally, the features need to be normalised.\n",
    "However, we found that after normalisation, the accuracy rate became smaller instead, which I suspect is due to the occurrence of over-fitting.\n",
    "The original data had different units and large gaps between features, which would make the data more scattered.\n",
    "After normalisation, the gap between features becomes smaller and overfitting can easily occur.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Kernel trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "The SVC(rbf) Accuracy is 98.33333333333333 % \n",
      "58\n",
      "The SVC(linear) Accuracy is 96.66666666666667 % \n",
      "58\n",
      "The SVC(poly) Accuracy is 96.66666666666667 % \n",
      "55\n",
      "The SVC(sigmoid) Accuracy is 91.66666666666666 % \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#rbf\n",
    "svm1 = SVC(kernel = 'rbf').fit(train_X,train_y)\n",
    "predict_test = svm1.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "print(correct_amount_km)\n",
    "print(\"The SVC(rbf) Accuracy is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))\n",
    "\n",
    "#linear\n",
    "svm2 = SVC(kernel = 'linear').fit(train_X,train_y)\n",
    "predict_test = svm2.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "print(correct_amount_km)\n",
    "print(\"The SVC(linear) Accuracy is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))\n",
    "\n",
    "#poly\n",
    "svm3 = SVC(kernel = 'poly').fit(train_X,train_y)\n",
    "predict_test = svm3.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "print(correct_amount_km)\n",
    "print(\"The SVC(poly) Accuracy is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))\n",
    "\n",
    "#sigmoid\n",
    "svm4 = SVC(kernel = 'sigmoid').fit(train_X,train_y)\n",
    "predict_test = svm4.predict(test_X)\n",
    "predict_test = pd.DataFrame(predict_test)\n",
    "pd_all = predict_test.join(test_y, lsuffix='_test_y', rsuffix='_predict_test')\n",
    "correct_amount_km = pd_all[pd_all[\"0_test_y\"] == pd_all[\"0_predict_test\"]].shape[0]\n",
    "print(correct_amount_km)\n",
    "print(\"The SVC(sigmoid) Accuracy is {0} % \".format(correct_amount_km / pd_all.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that the highest accuracy is achieved when the rbf parameter is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
